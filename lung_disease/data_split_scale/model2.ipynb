{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer, QuantileTransformer, PowerTransformer, FunctionTransformer,LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clear GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Num GPUs Available:  1\n",
      "Memory growth set successfully.\n",
      "Num GPUs Available:  [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Check TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "\n",
    "# Check for GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth for each GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set successfully.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs found.\")\n",
    "print(\"Num GPUs Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import clear_session\n",
    "clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonâ€™s garbage collector can help free up memory that is no longer in use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Mixed Precision Training\n",
    "Enable mixed precision training to reduce memory usage. TensorFlow's mixed precision API can help with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "# Set the policy for mixed precision\n",
    "#policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "#tf.keras.mixed_precision.set_global_policy(policy)\n",
    "\n",
    "# Confirm the policy is set\n",
    "#print(\"Mixed precision policy:\", tf.keras.mixed_precision.global_policy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust GPU Memory Allocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.10.0\n",
      "Num GPUs Available:  1\n",
      "Memory growth set successfully.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "# Check for GPUs\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(gpus))\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth for each GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Memory growth set successfully.\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPUs found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_loaded_data_x= np.load(r'E:\\projects\\paper\\myself\\data_processing\\x_data_non_downsampled.npz')\n",
    "reduced_loaded_data_y = np.load(r'E:\\projects\\paper\\myself\\data_processing\\y_data_non_downsampled.npz')\n",
    "\n",
    "x= reduced_loaded_data_x['x']\n",
    "y= reduced_loaded_data_y['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05343904,  0.10471563,  0.09451785, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.02075644, -0.0404014 , -0.03602667, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.00218729,  0.00407564,  0.00339912, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.00942281,  0.01889487,  0.01717605, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.05081851, -0.09910698, -0.08860061, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.05121048, -0.09565088, -0.07949052, ...,  0.        ,\n",
       "         0.        ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(917, 413760)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_shape=x.shape\n",
    "x_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 3, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2, 2, 2, 2, 2, 3, 3, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 3, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 5, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 5, 5, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3,\n",
       "       3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 1, 1, 1, 5, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 5, 5, 5, 5, 2, 2, 2, 2, 2, 1, 1, 0, 0,\n",
       "       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 5, 5, 5, 5, 2, 5,\n",
       "       4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 3, 3, 2, 2, 2, 2, 2, 2, 0, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 3, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 1,\n",
       "       1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       3, 3, 5, 5, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 0, 0, 1, 1, 3, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(917,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_shape=y.shape\n",
    "y_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_classes = np.unique(y)\n",
    "\n",
    "# Number of unique classes\n",
    "num_classes = len(unique_classes)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_split(X, y, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=random_state, stratify=y_train)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = stratified_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=lambda x : pd.Series(x).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    475\n",
       "4     22\n",
       "3     21\n",
       "5     13\n",
       "0     10\n",
       "1      8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    159\n",
       "4      8\n",
       "3      7\n",
       "5      5\n",
       "0      3\n",
       "1      2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    159\n",
       "3      7\n",
       "4      7\n",
       "5      5\n",
       "1      3\n",
       "0      3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom transformation functions\n",
    "def fourier_transform(data):\n",
    "    return np.fft.fft(data).real\n",
    "\n",
    "def inverse_fourier_transform(data):\n",
    "    return np.fft.ifft(data).real\n",
    "\n",
    "def log_transform(data):\n",
    "    return np.log1p(np.abs(data))\n",
    "\n",
    "def power_transform(data):\n",
    "    return np.power(data, 2)\n",
    "\n",
    "def stft_transform(data):\n",
    "    return np.abs(tf.signal.stft(data, frame_length=256, frame_step=128))\n",
    "\n",
    "# Define the preprocessing transformers\n",
    "def create_preprocessing_transformers():\n",
    "    return {\n",
    "        # 'standard': StandardScaler(),\n",
    "        # 'minmax': MinMaxScaler(),\n",
    "        # 'robust': RobustScaler(),\n",
    "        'normalize': Normalizer(),\n",
    "        # 'quantile': QuantileTransformer(),\n",
    "        # 'power': PowerTransformer(),\n",
    "        # 'fourier': FunctionTransformer(fourier_transform),\n",
    "        # 'inverse_fourier': FunctionTransformer(inverse_fourier_transform),\n",
    "        # 'log': FunctionTransformer(log_transform),\n",
    "        # 'custom_power': FunctionTransformer(power_transform),\n",
    "        # 'stft': FunctionTransformer(stft_transform)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def plot_data_distributions(data, labels, data_type, split_name, scaler_name, label_names):\n",
    "#     df = pd.DataFrame(data)\n",
    "    \n",
    "#     # Map numeric labels to their corresponding names\n",
    "#     df['label'] = [label_names[label] for label in labels]  # Use actual labels here\n",
    "\n",
    "#     # Combine all plotting into a single figure to reduce fragmentation\n",
    "#     fig, axs = plt.subplots(2, 2, figsize=(20, 12))\n",
    "\n",
    "#     # Histogram\n",
    "#     # print(\"histogram\")\n",
    "#     xp=df.iloc[:,0]\n",
    "#     # print(xp,\"\\n\\n\\n\")\n",
    "#     # print(df,\"\\n\\n\\n\")\n",
    "#     # sns.histplot(df, x=xp, hue='label', multiple='stack', ax=axs[0, 0])\n",
    "#     # axs[0, 0].set_title(f'{data_type} {split_name} Data Distribution - Histogram with {scaler_name}')\n",
    "\n",
    "#     # Violin plot\n",
    "#     print(\"violin\")\n",
    "#     sns.violinplot(x='label', y=xp, data=df, ax=axs[0, 1])\n",
    "#     axs[0, 1].set_title(f'{data_type} {split_name} Data Distribution - Violin Plot with {scaler_name}')\n",
    "\n",
    "#     # KDE plot (Distribution plot)\n",
    "#     print(\"kde\")\n",
    "#     sns.kdeplot(data=df, x=xp, hue='label', ax=axs[1, 0])\n",
    "#     axs[1, 0].set_title(f'{data_type} {split_name} Data Distribution - KDE Plot with {scaler_name}')\n",
    "#     # Apply log scale to y-axis\n",
    "#     axs[1, 0].set_yscale('log')\n",
    "\n",
    "#     print(\"line\")\n",
    "#     # Note: To plot lines, you need to aggregate or compute statistics like means or medians if there are multiple data points.\n",
    "#     sns.lineplot(x=df.index, y=df.columns[0], hue='label', data=df, ax=axs[1, 1])\n",
    "#     axs[1, 1].set_title(f'{data_type} {split_name} Data Distribution - Line Plot with {scaler_name}')\n",
    "\n",
    "\n",
    "#     print(f'{data_type}_{split_name}_{scaler_name} saving plot')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'plots/{data_type}_{split_name}_{scaler_name}_combined_plots.png')\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "# # Preprocess and save data\n",
    "# def preprocess_and_save_data(data_dict, labels_dict, data_type, label_names):\n",
    "#     transformers = create_preprocessing_transformers()\n",
    "#     errored_scalers = []\n",
    "\n",
    "#     os.makedirs('plots', exist_ok=True)\n",
    "#     os.makedirs('evaluation', exist_ok=True)\n",
    "\n",
    "#     for name, transformer in tqdm(transformers.items(), desc=f'Processing {data_type} data'):\n",
    "#         for split_name, data in data_dict.items():\n",
    "#             labels = labels_dict[split_name]  # Get labels for the current split\n",
    "#             save_path = f'E:\\\\projects\\\\paper\\\\myself\\\\scaling\\\\data\\\\x_{data_type}_{split_name}_{name}_transformed_downsampled.npz'\n",
    "#             if os.path.exists(save_path):\n",
    "#                 print(f\"File for {name} scaler on {split_name} data already exists. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             try:\n",
    "#                 print(f\"Processing with {name} scaler on {split_name} data\")\n",
    "#                 print('applying trans')\n",
    "#                 transformed_data = transformer.fit_transform(data)\n",
    "#                 np.savez_compressed(save_path, x=transformed_data)\n",
    "#                 print(f\"{name} scaler for {split_name} data finished and saved.\")\n",
    "#                 # if hasattr(transformer, 'scale_') and hasattr(transformer, 'min_'):\n",
    "#                 #     min_value = transformer.min_.min()\n",
    "#                 #     max_value = transformer.scale_.max()\n",
    "#                 #     with open(f'evaluation/{name}_scaler_log.txt', 'a') as f:\n",
    "#                 #         f.write(f\"{data_type} {split_name} - Min value: {min_value}, Max value: {max_value}\\n\")\n",
    "#                 #     print(f\"{name} scaler log updated with min and max values for {split_name}.\")\n",
    "#                 # else:\n",
    "#                 #     with open(f'evaluation/{name}_scaler_log.txt', 'a') as f:\n",
    "#                 #         f.write(f\"{data_type} {split_name} - Scalar has no min or max values\\n\")\n",
    "\n",
    "\n",
    "#                 # Plot distributions\n",
    "#                 plot_data_distributions(transformed_data, labels, data_type, split_name, name, label_names)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error with {name} scaler on {split_name} data: {e}\")\n",
    "#                 errored_scalers.append((name, split_name))\n",
    "\n",
    "#     if errored_scalers:\n",
    "#         print(f\"Scalers that failed: {errored_scalers}\")\n",
    "\n",
    "\n",
    "# def main(x, y):\n",
    "#     # Stratified split\n",
    "#     print('splitting')\n",
    "#     X_train, X_val, X_test, y_train, y_val, y_test = stratified_split(x, y)\n",
    "#     print('splitting successfully')\n",
    "#     data_dict = {\n",
    "#         'train': X_train,\n",
    "#         'val': X_val,\n",
    "#         'test': X_test\n",
    "#     }\n",
    "#     labels_dict = {\n",
    "#         'train': y_train,\n",
    "#         'val': y_val,\n",
    "#         'test': y_test\n",
    "#     }\n",
    "#     label_names = ['Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'Pneumonia', 'URTI']\n",
    "#     # Preprocess and save data\n",
    "#     preprocess_and_save_data(data_dict, labels_dict, 'data', label_names)\n",
    "\n",
    "#     print(\"Data preprocessed, saved, and plotted.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Load your data here\n",
    "#     x = x\n",
    "#     y = y  \n",
    "#     main(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN ALL MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=common_filter_count, kernel_size=3, input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(dropout_rate),\n",
    "        Conv1D(filters=common_filter_count, kernel_size=3),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(dropout_rate),\n",
    "        Flatten(),\n",
    "        Dense(common_dense_units),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_lstm_model(input_shape, num_classes):\n",
    "    normalization_layer = Normalization()\n",
    "    model = Sequential([\n",
    "        normalization_layer,\n",
    "        LSTM(common_rnn_units, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(dropout_rate),\n",
    "        Bidirectional(LSTM(common_rnn_units)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(common_dense_units),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_cnn_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=common_filter_count, kernel_size=3, input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(dropout_rate),\n",
    "        Bidirectional(LSTM(common_rnn_units)),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(common_dense_units),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def residual_block(x, filters, kernel_size=3):\n",
    "    shortcut = Conv1D(filters, kernel_size=1, padding='same')(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    LeakyReLU(alpha=0.1)(x)\n",
    "    x = Conv1D(filters, kernel_size, padding='same')(x)\n",
    "    x = Add()([x, shortcut])\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    return x\n",
    "\n",
    "def create_resnet_model(input_shape, num_classes):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x = Conv1D(common_filter_count, kernel_size=3, padding='same')(input_layer)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = residual_block(x, common_filter_count)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = residual_block(x, common_filter_count * 2)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(common_dense_units)(x)\n",
    "    x = LeakyReLU(alpha=0.1)(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output_layer = Dense(num_classes, activation='softmax')(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "def create_gru_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        GRU(common_rnn_units, return_sequences=True, input_shape=input_shape, recurrent_activation='sigmoid'),  # Set recurrent_activation\n",
    "        Dropout(dropout_rate),\n",
    "        GRU(common_rnn_units, recurrent_activation='sigmoid'),  # Set recurrent_activation\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(common_dense_units),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_cnn_gru_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=common_filter_count, kernel_size=3, input_shape=input_shape),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(dropout_rate),\n",
    "        GRU(common_rnn_units, return_sequences=True),\n",
    "        Dropout(dropout_rate),\n",
    "        GRU(common_rnn_units),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(common_dense_units),\n",
    "        LeakyReLU(alpha=0.1),\n",
    "        Dropout(dropout_rate),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model_functions = {\n",
    "    'cnn': create_cnn_model,\n",
    "    'lstm': create_lstm_model,\n",
    "    'cnn_lstm': create_cnn_lstm_model,\n",
    "    'resnet': create_resnet_model,\n",
    "    'gru': create_gru_model,\n",
    "    'cnn_gru': create_cnn_gru_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: (917, 413760)\n",
      "y shape: (917,)\n",
      "memory growth successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Dropout, Flatten, LSTM, GRU, Bidirectional, Input, Add, Activation\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler,MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Bidirectional, GRU, LeakyReLU, Add, Input, ZeroPadding1D, Normalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Example data (replace with actual data)\n",
    "\n",
    "\n",
    "def stratified_split(X, y, test_size=0.2, val_size=0.25, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=random_state, stratify=y_train)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# Check the shape of x and y\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = stratified_split(x, y)\n",
    "\n",
    "# Constants\n",
    "common_filter_count = 16\n",
    "common_dense_units = 64\n",
    "common_rnn_units = 64\n",
    "dropout_rate = 0.3\n",
    "results_csv = 'evaluation/metrics_summary.csv'\n",
    "log_file = 'evaluation/training_log.txt'\n",
    "batch_size = 16\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# GPU setup\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only allocate memory as needed on the first GPU\n",
    "        tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "        print(\"memory growth successfully\")\n",
    "    except RuntimeError as e:\n",
    "        logging.error(f\"Error setting memory growth: {e}\")\n",
    "\n",
    "# Define model creation functions\n",
    "# ... [Include model creation functions here, same as in the provided code]\n",
    "\n",
    "def create_tf_dataset(x, y, batch_size=batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "    dataset = dataset.shuffle(buffer_size=10000).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "def plot_and_save_results(history, model_name,name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{model_name} - Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(f'plots/{model_name}_{name}_accuracy_loss.png')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_and_save_results(name,model, x_test, y_test, model_name, data_type):\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "    precision = precision_score(y_test, y_pred_classes, average=None)\n",
    "    recall = recall_score(y_test, y_pred_classes, average=None)\n",
    "    f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "    cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    \n",
    "    # ROC Curve\n",
    "    y_test_bin = tf.keras.utils.to_categorical(y_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_pred.ravel())\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Save metrics to a file\n",
    "    with open(f'evaluation/{model_name}_{name}_metrics.txt', 'w') as f:\n",
    "        f.write(f\"Model: {model_name}_{name}\\n\")\n",
    "        f.write(f\"Data Type: {data_type}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"Precision:\\n{precision}\\n\")\n",
    "        f.write(f\"Recall:\\n{recall}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "        f.write(f\"Confusion Matrix:\\n{cm}\\n\")\n",
    "    \n",
    "    # Save key metrics to a CSV file\n",
    "    metrics_df = pd.DataFrame({\n",
    "        'Model': [model_name],\n",
    "        'trans': [name],\n",
    "        'Data_Type': [data_type],\n",
    "        'Accuracy': [accuracy],\n",
    "        'F1_Score': [f1],\n",
    "    })\n",
    "    if not os.path.exists(results_csv):\n",
    "        metrics_df.to_csv(results_csv, index=False)\n",
    "    else:\n",
    "        metrics_df.to_csv(results_csv, mode='a', header=False, index=False)\n",
    "\n",
    "def run_model_pipeline(x_train, y_train, x_val, y_val, x_test, y_test, model_name, input_shape, num_classes,name):\n",
    "    if model_name not in model_functions:\n",
    "        raise ValueError(f\"Model {model_name} {name} is not defined.\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"# Create model\")\n",
    "\n",
    "    model = model_functions[model_name](input_shape, num_classes)\n",
    "    \n",
    "    # Compile model\n",
    "    print(f\"# Compile model{model_name}{name}\")\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20),\n",
    "        ModelCheckpoint(filepath=f'checkpoints/{model_name}{name}.h5', save_best_only=True, monitor='val_loss')\n",
    "    ]\n",
    "\n",
    "    # Prepare datasets\n",
    "    print(\"# Prepare datasets\")\n",
    "    train_dataset = create_tf_dataset(x_train, y_train)\n",
    "    val_dataset = create_tf_dataset(x_val, y_val)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"# Train model\")\n",
    "    history = model.fit(train_dataset, validation_data=val_dataset, epochs=50, callbacks=callbacks, verbose=2)\n",
    "    \n",
    "    # Log CUDA device info every 20 epochs\n",
    "    print(\"# Log CUDA device info every 20 epochs\")\n",
    "    for epoch in tqdm(range(100), desc=f'Training {model_name}', unit='epoch'):\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            logging.info(f'Epoch {epoch + 1}: CUDA device info - {tf.config.list_physical_devices(\"GPU\")}')\n",
    "    \n",
    "    # Evaluate model\n",
    "    print(\"# Evaluate model\")\n",
    "    evaluate_and_save_results(name,model, x_test, y_test, model_name, 'test')\n",
    "\n",
    "    # Plot results\n",
    "    print(\"# Plot results\")\n",
    "    plot_and_save_results(history, model_name,name)\n",
    "\n",
    "    # Collect results\n",
    "    print(\"# Collect results\")\n",
    "    train_acc = history.history['accuracy'][-1]\n",
    "    val_acc = history.history['val_accuracy'][-1]\n",
    "    test_acc = accuracy_score(y_test, np.argmax(model.predict(x_test), axis=1))\n",
    "    f1 = f1_score(y_test, np.argmax(model.predict(x_test), axis=1), average='weighted')\n",
    "    \n",
    "    # Update results CSV\n",
    "    print(\"# Update results CSV\")\n",
    "    results_df = pd.read_csv(results_csv) if os.path.exists(results_csv) else pd.DataFrame(columns=['Model','name', 'Data_Type', 'Train_Accuracy', 'Validation_Accuracy', 'Test_Accuracy', 'F1_Score', 'Batch_Size'])\n",
    "    new_result = pd.DataFrame([[model_name,name, 'test', train_acc, val_acc, test_acc, f1, batch_size]], columns=results_df.columns)\n",
    "    results_df = pd.concat([results_df, new_result], ignore_index=True)\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(x,y_train,y_val,y_test,x_train):\n",
    "#     os.makedirs('plots', exist_ok=True)\n",
    "#     os.makedirs('checkpoints', exist_ok=True)\n",
    "#     os.makedirs('evaluation', exist_ok=True)\n",
    "    \n",
    "#     # Define model parameters\n",
    "#     num_classes = len(np.unique(y))\n",
    "#     print(num_classes)\n",
    "\n",
    "#     # Define transformers\n",
    "#     transformers = create_preprocessing_transformers()\n",
    "    \n",
    "#     # Iterate over each transformer\n",
    "#     for name, transformer in transformers.items():\n",
    "#         print(f\"Processing with {name} transformer\")\n",
    "        \n",
    "#           # Load preprocessed data for each transformer\n",
    "#         x_train = np.load(f'data/x_data_train_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_train = y_train  # Labels are typically not transformed\n",
    "#         x_val = np.load(f'data/x_data_val_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_val = y_val  # Labels are typically not transformed\n",
    "#         x_test = np.load(f'data/x_data_test_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_test = y_test  # Labels are typically not transformed\n",
    "\n",
    "\n",
    "#         input_shape = (x_train.shape[1], 1)\n",
    "        \n",
    "        \n",
    "#         # Ensure correct shape of data\n",
    "#         print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "#         print(f\"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}\")\n",
    "#         print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "        \n",
    "#         # Train and evaluate models\n",
    "#         model_name = 'cnn'\n",
    "\n",
    "#         try:\n",
    "#             run_model_pipeline(x_train, y_train, x_val, y_val, x_test, y_test, model_name, input_shape, num_classes,name)\n",
    "#             logging.info(f\"Completed {model_name} with {name} transformer\")\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "#             print(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "#         clear_session()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(x,y_train,y_val,y_test,X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def main(x,y_train,y_val,y_test,x_train):\n",
    "#     os.makedirs('plots', exist_ok=True)\n",
    "#     os.makedirs('checkpoints', exist_ok=True)\n",
    "#     os.makedirs('evaluation', exist_ok=True)\n",
    "    \n",
    "#      # Define model parameters\n",
    "#     num_classes = len(np.unique(y))\n",
    "#     print(num_classes)\n",
    "    \n",
    "#     # Define transformers\n",
    "#     transformers = create_preprocessing_transformers()\n",
    "    \n",
    "#     # Iterate over each transformer\n",
    "#     for name, transformer in transformers.items():\n",
    "#         print(f\"Processing with {name} transformer\")\n",
    "        \n",
    "#          # Load preprocessed data for each transformer\n",
    "#         x_train = np.load(f'data/x_data_train_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_train = y_train  # Labels are typically not transformed\n",
    "#         x_val = np.load(f'data/x_data_val_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_val = y_val  # Labels are typically not transformed\n",
    "#         x_test = np.load(f'data/x_data_test_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_test = y_test  # Labels are typically not transformed\n",
    "\n",
    "#         input_shape = (x_train.shape[1], 1)\n",
    "\n",
    "        \n",
    "#         # Ensure correct shape of data\n",
    "#         print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "#         print(f\"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}\")\n",
    "#         print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "#         # Example data reshaping for LSTM\n",
    "#         x_train = x_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "#         x_val = x_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "#         x_test = x_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "\n",
    "#         # Ensure correct shape of data\n",
    "#         print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "#         print(f\"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}\")\n",
    "#         print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "#         # Train and evaluate only the CNN model\n",
    "#         model_name = 'lstm'\n",
    "#         try:\n",
    "#             run_model_pipeline(x_train, y_train, x_val, y_val, x_test, y_test, model_name, input_shape, num_classes,name)\n",
    "#             logging.info(f\"Completed {model_name} with {name} transformer\")\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "#             print(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "#             continue\n",
    "#         clear_session()\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(x,y_train,y_val,y_test,X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(x, y_train, y_val, y_test, X_train):\n",
    "#     os.makedirs('plots', exist_ok=True)\n",
    "#     os.makedirs('checkpoints', exist_ok=True)\n",
    "#     os.makedirs('evaluation', exist_ok=True)\n",
    "    \n",
    "#     # Define model parameters\n",
    "#     num_classes = len(np.unique(y_train))\n",
    "#     print(\"Number of classes:\", num_classes)\n",
    "    \n",
    "#     # Define transformers\n",
    "#     transformers = create_preprocessing_transformers()\n",
    "    \n",
    "#     # Iterate over each transformer\n",
    "#     for name, transformer in transformers.items():\n",
    "#         print(f\"Processing with {name} transformer\")\n",
    "        \n",
    "#         # Load preprocessed data for each transformer\n",
    "#         x_train = np.load(f'data/x_data_train_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_train = y_train  # Labels are typically not transformed\n",
    "#         x_val = np.load(f'data/x_data_val_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_val = y_val  # Labels are typically not transformed\n",
    "#         x_test = np.load(f'data/x_data_test_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_test = y_test  # Labels are typically not transformed\n",
    "\n",
    "#         # Define input shape for the model\n",
    "#         input_shape = (x_train.shape[1], 1)\n",
    "#         print(\"Input shape:\", input_shape)\n",
    "\n",
    "#         # Ensure correct shape of data before reshaping\n",
    "#         print(f\"x_train shape before reshaping: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "#         print(f\"x_val shape before reshaping: {x_val.shape}, y_val shape: {y_val.shape}\")\n",
    "#         print(f\"x_test shape before reshaping: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "#         # Reshape data for GRU model: (batch_size, timesteps, features)\n",
    "#         x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))\n",
    "#         x_val = x_val.reshape((x_val.shape[0], x_val.shape[1], 1))\n",
    "#         x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "#         # Ensure correct shape of data after reshaping\n",
    "#         print(f\"x_train shape after reshaping: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "#         print(f\"x_val shape after reshaping: {x_val.shape}, y_val shape: {y_val.shape}\")\n",
    "#         print(f\"x_test shape after reshaping: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "#         model_name = 'cnn_lstm'\n",
    "\n",
    "#         print(\"model_name\",model_name)\n",
    "#         try:\n",
    "#             run_model_pipeline(x_train, y_train, x_val, y_val, x_test, y_test, model_name, input_shape, num_classes, name)\n",
    "#             logging.info(f\"Completed {model_name} with {name} transformer\")\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "#             print(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "#             continue\n",
    "        \n",
    "#         clear_session()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(x, y_train, y_val, y_test, X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "Processing with normalize transformer\n",
      "x_train shape: (549, 413760), y_train shape: (549,)\n",
      "x_val shape: (184, 413760), y_val shape: (184,)\n",
      "x_test shape: (184, 413760), y_test shape: (184,)\n",
      "# Create model\n",
      "# Compile modelresnetnormalize\n",
      "# Prepare datasets\n",
      "# Train model\n",
      "Epoch 1/50\n",
      "35/35 - 35s - loss: 2.4012 - accuracy: 0.6995 - val_loss: 1.8755 - val_accuracy: 0.8641 - lr: 0.0010 - 35s/epoch - 990ms/step\n",
      "Epoch 2/50\n",
      "35/35 - 30s - loss: 1.2273 - accuracy: 0.7832 - val_loss: 0.6583 - val_accuracy: 0.8641 - lr: 0.0010 - 30s/epoch - 852ms/step\n",
      "Epoch 3/50\n",
      "35/35 - 29s - loss: 0.7428 - accuracy: 0.8525 - val_loss: 0.6343 - val_accuracy: 0.8641 - lr: 0.0010 - 29s/epoch - 837ms/step\n",
      "Epoch 4/50\n",
      "35/35 - 12s - loss: 0.7243 - accuracy: 0.8652 - val_loss: 0.6455 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 355ms/step\n",
      "Epoch 5/50\n",
      "35/35 - 12s - loss: 0.7278 - accuracy: 0.8634 - val_loss: 0.6626 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 352ms/step\n",
      "Epoch 6/50\n",
      "35/35 - 12s - loss: 0.7139 - accuracy: 0.8652 - val_loss: 0.6731 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 352ms/step\n",
      "Epoch 7/50\n",
      "35/35 - 12s - loss: 0.6984 - accuracy: 0.8652 - val_loss: 0.6618 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 353ms/step\n",
      "Epoch 8/50\n",
      "35/35 - 30s - loss: 0.7717 - accuracy: 0.8616 - val_loss: 0.6151 - val_accuracy: 0.8641 - lr: 0.0010 - 30s/epoch - 855ms/step\n",
      "Epoch 9/50\n",
      "35/35 - 13s - loss: 0.7592 - accuracy: 0.8616 - val_loss: 0.6594 - val_accuracy: 0.8641 - lr: 0.0010 - 13s/epoch - 361ms/step\n",
      "Epoch 10/50\n",
      "35/35 - 12s - loss: 0.7148 - accuracy: 0.8579 - val_loss: 0.6199 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 353ms/step\n",
      "Epoch 11/50\n",
      "35/35 - 12s - loss: 0.6876 - accuracy: 0.8652 - val_loss: 0.7558 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 353ms/step\n",
      "Epoch 12/50\n",
      "35/35 - 12s - loss: 0.7126 - accuracy: 0.8652 - val_loss: 0.6217 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 354ms/step\n",
      "Epoch 13/50\n",
      "35/35 - 12s - loss: 0.6842 - accuracy: 0.8652 - val_loss: 0.6269 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 354ms/step\n",
      "Epoch 14/50\n",
      "35/35 - 12s - loss: 0.6608 - accuracy: 0.8652 - val_loss: 0.8222 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 353ms/step\n",
      "Epoch 15/50\n",
      "35/35 - 12s - loss: 0.6669 - accuracy: 0.8652 - val_loss: 0.6222 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 353ms/step\n",
      "Epoch 16/50\n",
      "35/35 - 12s - loss: 0.6556 - accuracy: 0.8634 - val_loss: 0.6180 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 354ms/step\n",
      "Epoch 17/50\n",
      "35/35 - 12s - loss: 6.7000 - accuracy: 0.7505 - val_loss: 3.4612 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 355ms/step\n",
      "Epoch 18/50\n",
      "35/35 - 12s - loss: 3.5254 - accuracy: 0.7413 - val_loss: 1.2116 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 356ms/step\n",
      "Epoch 19/50\n",
      "35/35 - 12s - loss: 1.4637 - accuracy: 0.7760 - val_loss: 0.8586 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 356ms/step\n",
      "Epoch 20/50\n",
      "35/35 - 12s - loss: 1.4380 - accuracy: 0.7486 - val_loss: 2.8857 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 356ms/step\n",
      "Epoch 21/50\n",
      "35/35 - 12s - loss: 2.4274 - accuracy: 0.7632 - val_loss: 1.7341 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 357ms/step\n",
      "Epoch 22/50\n",
      "35/35 - 12s - loss: 3.1031 - accuracy: 0.7559 - val_loss: 2.2483 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 357ms/step\n",
      "Epoch 23/50\n",
      "35/35 - 13s - loss: 1.8084 - accuracy: 0.7486 - val_loss: 0.8731 - val_accuracy: 0.8641 - lr: 0.0010 - 13s/epoch - 358ms/step\n",
      "Epoch 24/50\n",
      "35/35 - 12s - loss: 1.5156 - accuracy: 0.7687 - val_loss: 1.4263 - val_accuracy: 0.8641 - lr: 0.0010 - 12s/epoch - 357ms/step\n",
      "Epoch 25/50\n",
      "35/35 - 13s - loss: 1.5532 - accuracy: 0.7832 - val_loss: 0.7501 - val_accuracy: 0.8641 - lr: 0.0010 - 13s/epoch - 357ms/step\n",
      "Epoch 26/50\n",
      "35/35 - 13s - loss: 1.1162 - accuracy: 0.8033 - val_loss: 1.3255 - val_accuracy: 0.8641 - lr: 0.0010 - 13s/epoch - 358ms/step\n",
      "Epoch 27/50\n",
      "35/35 - 13s - loss: 1.4645 - accuracy: 0.8015 - val_loss: 1.0613 - val_accuracy: 0.8641 - lr: 0.0010 - 13s/epoch - 358ms/step\n",
      "Epoch 28/50\n",
      "35/35 - 13s - loss: 1.3601 - accuracy: 0.7887 - val_loss: 0.9241 - val_accuracy: 0.8641 - lr: 0.0010 - 13s/epoch - 358ms/step\n",
      "Epoch 29/50\n",
      "35/35 - 13s - loss: 1.1169 - accuracy: 0.7996 - val_loss: 0.7986 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 357ms/step\n",
      "Epoch 30/50\n",
      "35/35 - 13s - loss: 0.9535 - accuracy: 0.8087 - val_loss: 0.7234 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "Epoch 31/50\n",
      "35/35 - 13s - loss: 0.9047 - accuracy: 0.8379 - val_loss: 0.6998 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "Epoch 32/50\n",
      "35/35 - 13s - loss: 0.8449 - accuracy: 0.8233 - val_loss: 0.6981 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "Epoch 33/50\n",
      "35/35 - 13s - loss: 0.8380 - accuracy: 0.8506 - val_loss: 0.6531 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 357ms/step\n",
      "Epoch 34/50\n",
      "35/35 - 13s - loss: 0.8748 - accuracy: 0.8342 - val_loss: 0.6348 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "Epoch 35/50\n",
      "35/35 - 13s - loss: 0.8050 - accuracy: 0.8361 - val_loss: 0.6471 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "Epoch 36/50\n",
      "35/35 - 13s - loss: 0.8217 - accuracy: 0.8543 - val_loss: 0.6217 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 357ms/step\n",
      "Epoch 37/50\n",
      "35/35 - 13s - loss: 0.7557 - accuracy: 0.8488 - val_loss: 0.6374 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "Epoch 38/50\n",
      "35/35 - 13s - loss: 0.7406 - accuracy: 0.8506 - val_loss: 0.6474 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "Epoch 39/50\n",
      "35/35 - 13s - loss: 0.7347 - accuracy: 0.8616 - val_loss: 0.6164 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "Epoch 40/50\n",
      "35/35 - 13s - loss: 0.7202 - accuracy: 0.8525 - val_loss: 0.6163 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "Epoch 41/50\n",
      "35/35 - 29s - loss: 0.6953 - accuracy: 0.8634 - val_loss: 0.6087 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 29s/epoch - 838ms/step\n",
      "Epoch 42/50\n",
      "35/35 - 13s - loss: 0.7433 - accuracy: 0.8579 - val_loss: 0.6211 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 359ms/step\n",
      "Epoch 43/50\n",
      "35/35 - 12s - loss: 0.7008 - accuracy: 0.8652 - val_loss: 0.6215 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 12s/epoch - 353ms/step\n",
      "Epoch 44/50\n",
      "35/35 - 12s - loss: 0.6521 - accuracy: 0.8652 - val_loss: 0.6124 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 12s/epoch - 355ms/step\n",
      "Epoch 45/50\n",
      "35/35 - 12s - loss: 0.6855 - accuracy: 0.8634 - val_loss: 0.6123 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 12s/epoch - 357ms/step\n",
      "Epoch 46/50\n",
      "35/35 - 13s - loss: 0.6769 - accuracy: 0.8652 - val_loss: 0.6102 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 357ms/step\n",
      "Epoch 47/50\n",
      "35/35 - 13s - loss: 0.7002 - accuracy: 0.8597 - val_loss: 0.6246 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "Epoch 48/50\n",
      "35/35 - 13s - loss: 0.6702 - accuracy: 0.8634 - val_loss: 0.6161 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 357ms/step\n",
      "Epoch 49/50\n",
      "35/35 - 13s - loss: 0.6850 - accuracy: 0.8652 - val_loss: 0.6144 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "Epoch 50/50\n",
      "35/35 - 13s - loss: 0.6803 - accuracy: 0.8652 - val_loss: 0.6156 - val_accuracy: 0.8641 - lr: 1.0000e-04 - 13s/epoch - 358ms/step\n",
      "# Log CUDA device info every 20 epochs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training resnet: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:00<00:00, 2562.00epoch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Evaluate model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 2s 281ms/step\n",
      "# Plot results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ahamed\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Collect results\n",
      "6/6 [==============================] - 1s 229ms/step\n",
      "6/6 [==============================] - 1s 228ms/step\n",
      "# Update results CSV\n",
      "Error in resnet with normalize transformer: 5 columns passed, passed data had 8 columns\n"
     ]
    }
   ],
   "source": [
    "def main(x,y_train,y_val,y_test,x_train):\n",
    "    os.makedirs('plots', exist_ok=True)\n",
    "    os.makedirs('checkpoints', exist_ok=True)\n",
    "    os.makedirs('evaluation', exist_ok=True)\n",
    "    \n",
    "    # Define model parameters\n",
    "    num_classes = len(np.unique(y))\n",
    "    print(num_classes)\n",
    "\n",
    "    # Define transformers\n",
    "    transformers = create_preprocessing_transformers()\n",
    "    \n",
    "    # Iterate over each transformer\n",
    "    for name, transformer in transformers.items():\n",
    "        print(f\"Processing with {name} transformer\")\n",
    "        \n",
    "          # Load preprocessed data for each transformer\n",
    "        x_train = np.load(f'data/x_data_train_{name}_transformed_downsampled.npz')['x']\n",
    "        y_train = y_train  # Labels are typically not transformed\n",
    "        x_val = np.load(f'data/x_data_val_{name}_transformed_downsampled.npz')['x']\n",
    "        y_val = y_val  # Labels are typically not transformed\n",
    "        x_test = np.load(f'data/x_data_test_{name}_transformed_downsampled.npz')['x']\n",
    "        y_test = y_test  # Labels are typically not transformed\n",
    "\n",
    "\n",
    "        input_shape = (x_train.shape[1], 1)\n",
    "        \n",
    "        \n",
    "        # Ensure correct shape of data\n",
    "        print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "        print(f\"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}\")\n",
    "        print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "\n",
    "        \n",
    "        # Train and evaluate models\n",
    "        model_name = 'resnet'\n",
    "\n",
    "        try:\n",
    "            run_model_pipeline(x_train, y_train, x_val, y_val, x_test, y_test, model_name, input_shape, num_classes,name)\n",
    "            logging.info(f\"Completed {model_name} with {name} transformer\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "            print(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "        clear_session()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(x,y_train,y_val,y_test,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "# from tensorflow.keras.models import Sequential, Model\n",
    "# from tensorflow.keras.layers import Dense, Conv1D, MaxPooling1D, Dropout, Flatten, LSTM, GRU, Bidirectional, Input, Add, Activation\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, roc_curve, auc\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from tqdm import tqdm\n",
    "# import logging\n",
    "\n",
    "# from os import listdir\n",
    "# from os.path import join\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler,MinMaxScaler\n",
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Bidirectional, GRU, LeakyReLU, Add, Input, ZeroPadding1D, Normalization\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.backend import clear_session\n",
    "\n",
    "# # Example data (replace with actual data)\n",
    "\n",
    "\n",
    "# def stratified_split(X, y, test_size=0.2, val_size=0.25, random_state=42):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=random_state, stratify=y_train)\n",
    "#     return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# # Check the shape of x and y\n",
    "# print(f\"x shape: {x.shape}\")\n",
    "# print(f\"y shape: {y.shape}\")\n",
    "\n",
    "# # Stratified split\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = stratified_split(x, y)\n",
    "\n",
    "# # Constants\n",
    "# common_filter_count = 16\n",
    "# common_dense_units = 32\n",
    "# common_rnn_units = 32\n",
    "# dropout_rate = 0.3\n",
    "# results_csv = 'evaluation/metrics_summary.csv'\n",
    "# os.makedirs(os.path.dirname(results_csv), exist_ok=True)\n",
    "# log_file = 'evaluation/training_log.txt'\n",
    "# batch_size = 8\n",
    "\n",
    "# # Set up logging\n",
    "# logging.basicConfig(filename=log_file, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "# def preprocess_and_save_data(data_dict, data_type):\n",
    "#     transformers = create_preprocessing_transformers()\n",
    "#     errored_scalers = []\n",
    "\n",
    "#     os.makedirs('plots', exist_ok=True)\n",
    "#     os.makedirs('evaluation', exist_ok=True)\n",
    "\n",
    "#     for name, transformer in tqdm(transformers.items(), desc=f'Processing {data_type} data'):\n",
    "#         for split_name, data in data_dict.items():\n",
    "#             save_path = f'E:\\\\projects\\\\paper\\\\myself\\\\scaling\\\\data\\\\x_{data_type}_{split_name}_{name}_transformed_downsampled.npz'\n",
    "#             if os.path.exists(save_path):\n",
    "#                 print(f\"File for {name} scaler on {split_name} data already exists. Skipping.\")\n",
    "#                 continue\n",
    "\n",
    "#             try:\n",
    "#                 print(f\"Processing with {name} scaler on {split_name} data\")\n",
    "#                 transformed_data = transformer.fit_transform(data)\n",
    "#                 np.savez_compressed(save_path, x=transformed_data)\n",
    "#                 print(f\"{name} scaler for {split_name} data finished and saved.\")\n",
    "\n",
    "#                 # Log min-max values for scalers\n",
    "#                 if hasattr(transformer, 'scale_') and hasattr(transformer, 'min_'):\n",
    "#                     min_value = transformer.min_.min()\n",
    "#                     max_value = transformer.scale_.max()\n",
    "#                     with open(f'evaluation/{name}_scaler_log.txt', 'a') as f:\n",
    "#                         f.write(f\"{data_type} {split_name} - Min value: {min_value}, Max value: {max_value}\\n\")\n",
    "#                     print(f\"{name} scaler log updated with min and max values for {split_name}.\")\n",
    "#                 else:\n",
    "#                     with open(f'evaluation/{name}_scaler_log.txt', 'a') as f:\n",
    "#                         f.write(f\"{data_type} {split_name} - Scalar has no min or max values\\n\")\n",
    "\n",
    "#                 # Plot distributions\n",
    "#                 plot_data_distributions(transformed_data, data_type, split_name, name)\n",
    "\n",
    "#             except Exception as e:\n",
    "#                 print(f\"Error with {name} scaler on {split_name} data: {e}\")\n",
    "#                 errored_scalers.append((name, split_name))\n",
    "\n",
    "#     if errored_scalers:\n",
    "#         print(f\"Scalers that failed: {errored_scalers}\")\n",
    "\n",
    "# def plot_data_distributions(data, data_type, split_name, scaler_name):\n",
    "#     df = pd.DataFrame(data)\n",
    "#     df['label'] = np.random.randint(0, 2, size=len(df))  # Dummy labels for plotting\n",
    "\n",
    "#     # Histogram\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.histplot(df, multiple='stack')\n",
    "#     plt.title(f'{data_type} {split_name} Data Distribution - Histogram with {scaler_name}')\n",
    "#     plt.savefig(f'plots/{data_type}_{split_name}_{scaler_name}_histogram.png')\n",
    "#     plt.close()\n",
    "\n",
    "#     # Violin plot\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.violinplot(x='label', y=df.columns[0], data=df)\n",
    "#     plt.title(f'{data_type} {split_name} Data Distribution - Violin Plot with {scaler_name}')\n",
    "#     plt.savefig(f'plots/{data_type}_{split_name}_{scaler_name}_violin.png')\n",
    "#     plt.close()\n",
    "\n",
    "#     # Distribution plot\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.displot(df, x=df.columns[0], kind='kde')\n",
    "#     plt.title(f'{data_type} {split_name} Data Distribution - KDE Plot with {scaler_name}')\n",
    "#     plt.savefig(f'plots/{data_type}_{split_name}_{scaler_name}_distplot.png')\n",
    "#     plt.close()\n",
    "\n",
    "#     # Box plot\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.boxplot(x='label', y=df.columns[0], data=df)\n",
    "#     plt.title(f'{data_type} {split_name} Data Distribution - Box Plot with {scaler_name}')\n",
    "#     plt.savefig(f'plots/{data_type}_{split_name}_{scaler_name}_boxplot.png')\n",
    "#     plt.close()\n",
    "\n",
    "\n",
    "# def stratified_split(X, y, test_size=0.2, val_size=0.25, random_state=42):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size, random_state=random_state, stratify=y_train)\n",
    "#     return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "# def create_tf_dataset(x, y, batch_size=batch_size):\n",
    "#     dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "#     dataset = dataset.shuffle(buffer_size=10000).batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "#     return dataset\n",
    "\n",
    "# def plot_and_save_results(history, model_name,name):\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "\n",
    "#     # Plot accuracy\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "#     plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "#     plt.title(f'{model_name} - Accuracy')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Accuracy')\n",
    "#     plt.legend()\n",
    "\n",
    "#     # Plot loss\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(history.history['loss'], label='Train Loss')\n",
    "#     plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "#     plt.title(f'{model_name} - Loss')\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.legend()\n",
    "\n",
    "#     plt.savefig(f'plots/{model_name}_{name}_accuracy_loss.png')\n",
    "#     plt.close()\n",
    "\n",
    "# def evaluate_and_save_results(name,model, x_test, y_test, model_name, data_type):\n",
    "#     y_pred = model.predict(x_test)\n",
    "#     y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "#     precision = precision_score(y_test, y_pred_classes, average=None)\n",
    "#     recall = recall_score(y_test, y_pred_classes, average=None)\n",
    "#     f1 = f1_score(y_test, y_pred_classes, average='weighted')\n",
    "#     cm = confusion_matrix(y_test, y_pred_classes)\n",
    "    \n",
    "#     # ROC Curve\n",
    "#     y_test_bin = tf.keras.utils.to_categorical(y_test)\n",
    "#     fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_pred.ravel())\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "#     # Save metrics to a file\n",
    "#     with open(f'evaluation/{model_name}_{name}_metrics.txt', 'w') as f:\n",
    "#         f.write(f\"Model: {model_name}_{name}\\n\")\n",
    "#         f.write(f\"Data Type: {data_type}\\n\")\n",
    "#         f.write(f\"Accuracy: {accuracy:.4f}\\n\")\n",
    "#         f.write(f\"Precision:\\n{precision}\\n\")\n",
    "#         f.write(f\"Recall:\\n{recall}\\n\")\n",
    "#         f.write(f\"F1 Score: {f1:.4f}\\n\")\n",
    "#         f.write(f\"Confusion Matrix:\\n{cm}\\n\")\n",
    "#         f.write(\"\\n\\n\\n\\n\\n\")\n",
    "    \n",
    "#     # Plot confusion matrix\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "#     plt.title(f'{model_name} {name}- Confusion Matrix')\n",
    "#     plt.xlabel('Predicted')\n",
    "#     plt.ylabel('True')\n",
    "#     plt.savefig(f'plots/{model_name}_{name}_confusion_matrix.png')\n",
    "#     plt.close()\n",
    "\n",
    "#     # Save key metrics to a CSV file\n",
    "#     metrics_df = pd.DataFrame({\n",
    "#         'Model': [model_name],\n",
    "#         'trans': [name],\n",
    "#         'Data_Type': [data_type],\n",
    "#         'Accuracy': [accuracy],\n",
    "#         'F1_Score': [f1],\n",
    "#     })\n",
    "#     if not os.path.exists(results_csv):\n",
    "#         metrics_df.to_csv(results_csv, index=False)\n",
    "#     else:\n",
    "#         metrics_df.to_csv(results_csv, mode='a', header=False, index=False)\n",
    "# def run_model_pipeline(x_train, y_train, x_val, y_val, x_test, y_test, model_name, input_shape, num_classes,name):\n",
    "#     if model_name not in model_functions:\n",
    "#         raise ValueError(f\"Model {model_name} {name} is not defined.\")\n",
    "    \n",
    "#     print(\"# Create model\")\n",
    "#     model = model_functions[model_name](input_shape, num_classes)\n",
    "    \n",
    "#     print(f\"# Compile model{model_name}{name}\")\n",
    "#     model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     print(\"# Define callbacks\")\n",
    "#     callbacks = [\n",
    "#         EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),\n",
    "#         ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20),\n",
    "#         ModelCheckpoint(filepath=f'checkpoints/{model_name}{name}.h5', save_best_only=True, monitor='val_loss')\n",
    "#     ]\n",
    "\n",
    "#     print(\"# Prepare datasets\")\n",
    "#     train_dataset = create_tf_dataset(x_train, y_train)\n",
    "#     val_dataset = create_tf_dataset(x_val, y_val)\n",
    "    \n",
    "#     print(\"# Train model\")\n",
    "#     history = model.fit(train_dataset, validation_data=val_dataset, epochs=50, callbacks=callbacks, verbose=2)\n",
    "    \n",
    "#     print(\"# Log CUDA device info every 20 epochs\")\n",
    "#     for epoch in tqdm(range(100), desc=f'Training {model_name}', unit='epoch'):\n",
    "#         if (epoch + 1) % 20 == 0:\n",
    "#             logging.info(f'Epoch {epoch + 1}: CUDA device info - {tf.config.list_physical_devices(\"GPU\")}')\n",
    "    \n",
    "#     print(\"# Evaluate model\")\n",
    "#     evaluate_and_save_results(name,model, x_test, y_test, model_name, 'test')\n",
    "\n",
    "#     print(\"# Plot results\")\n",
    "#     plot_and_save_results(history, model_name,name)\n",
    "\n",
    "#     print(\"# Collect results\")\n",
    "#     train_acc = history.history['accuracy'][-1]\n",
    "#     val_acc = history.history['val_accuracy'][-1]\n",
    "#     test_acc = accuracy_score(y_test, np.argmax(model.predict(x_test), axis=1))\n",
    "#     f1 = f1_score(y_test, np.argmax(model.predict(x_test), axis=1), average='weighted')\n",
    "    \n",
    "#     print(\"# Update results CSV\")\n",
    "#     results_df = pd.read_csv(results_csv) if os.path.exists(results_csv) else pd.DataFrame(columns=['Model','name', 'Data_Type', 'Train_Accuracy', 'Validation_Accuracy', 'Test_Accuracy', 'F1_Score', 'Batch_Size'])\n",
    "#     new_result = pd.DataFrame([[model_name,name, 'test', 'relu', train_acc, val_acc, test_acc, f1, batch_size]], columns=results_df.columns)\n",
    "#     results_df = pd.concat([results_df, new_result], ignore_index=True)\n",
    "#     results_df.to_csv(results_csv, index=False)\n",
    "\n",
    "# def main(x,y_train,y_val,y_test,x_train):\n",
    "#     os.makedirs('plots', exist_ok=True)\n",
    "#     os.makedirs('checkpoints', exist_ok=True)\n",
    "#     os.makedirs('evaluation', exist_ok=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Define transformers\n",
    "#     transformers = create_preprocessing_transformers()\n",
    "    \n",
    "#     # Iterate over each transformer\n",
    "#     for name, transformer in transformers.items():\n",
    "#         print(f\"Processing with {name} transformer\")\n",
    "                \n",
    "#         # Define model parameters\n",
    "#         num_classes = len(np.unique(y))\n",
    "#         print(num_classes)\n",
    "        \n",
    "        \n",
    "#          # Load preprocessed data for each transformer\n",
    "#         x_train = np.load(f'data/x_data_train_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_train = y_train  # Labels are typically not transformed\n",
    "#         x_val = np.load(f'data/x_data_val_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_val = y_val  # Labels are typically not transformed\n",
    "#         x_test = np.load(f'data/x_data_test_{name}_transformed_downsampled.npz')['x']\n",
    "#         y_test = y_test  # Labels are typically not transformed\n",
    "\n",
    "#         input_shape = (x_train.shape[1], 1)\n",
    "        \n",
    "#         # Ensure correct shape of data\n",
    "#         print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "#         print(f\"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}\")\n",
    "#         print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "#         # Example data reshaping for LSTM\n",
    "#         x_train = x_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "#         x_val = x_val.reshape((X_val.shape[0], 1, X_val.shape[1]))\n",
    "#         x_test = x_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "#         num_classes = len(np.unique(y))\n",
    "        \n",
    "#         # Ensure correct shape of data\n",
    "#         print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "#         print(f\"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}\")\n",
    "#         print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "        \n",
    "#         # Train and evaluate only the CNN model\n",
    "#         model_name = 'gru'\n",
    "#         try:\n",
    "#             run_model_pipeline(x_train, y_train, x_val, y_val, x_test, y_test, model_name, input_shape, num_classes,name)\n",
    "#             logging.info(f\"Completed {model_name} with {name} transformer\")\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "#             print(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "#             continue\n",
    "#         clear_session()\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(x,y_train,y_val,y_test,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(x, y_train, y_val, y_test, x_train):\n",
    "#     os.makedirs('plots', exist_ok=True)\n",
    "#     os.makedirs('checkpoints', exist_ok=True)\n",
    "#     os.makedirs('evaluation', exist_ok=True)\n",
    "    \n",
    "#     # Define model parameters\n",
    "#     num_classes = len(np.unique(y_train))\n",
    "#     print(num_classes)\n",
    "    \n",
    "#     # Define transformers\n",
    "#     transformers = create_preprocessing_transformers()\n",
    "    \n",
    "#     # Iterate over each transformer\n",
    "#     for name, transformer in transformers.items():\n",
    "#         print(f\"Processing with {name} transformer\")\n",
    "        \n",
    "#         try:\n",
    "#             # Load preprocessed data for each transformer\n",
    "#             x_train = np.load(f'data/x_data_train_{name}_transformed_downsampled.npz')['x']\n",
    "#             x_val = np.load(f'data/x_data_val_{name}_transformed_downsampled.npz')['x']\n",
    "#             x_test = np.load(f'data/x_data_test_{name}_transformed_downsampled.npz')['x']\n",
    "\n",
    "#             # Ensure correct shape of data\n",
    "#             print(f\"x_train shape: {x_train.shape}, y_train shape: {y_train.shape}\")\n",
    "#             print(f\"x_val shape: {x_val.shape}, y_val shape: {y_val.shape}\")\n",
    "#             print(f\"x_test shape: {x_test.shape}, y_test shape: {y_test.shape}\")\n",
    "            \n",
    "#             input_shape = (x_train.shape[1], 1)\n",
    "#             model_name = 'gru'\n",
    "            \n",
    "#             run_model_pipeline(x_train, y_train, x_val, y_val, x_test, y_test, model_name, input_shape, num_classes, name)\n",
    "#             logging.info(f\"Completed {model_name} with {name} transformer\")\n",
    "        \n",
    "#         except tf.errors.ResourceExhaustedError as e:\n",
    "#             logging.error(f\"ResourceExhaustedError in {model_name} with {name} transformer: {e}\")\n",
    "#             print(f\"ResourceExhaustedError in {model_name} with {name} transformer: {e}\")\n",
    "#             # You can try to reduce the batch size here and retry if needed.\n",
    "        \n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "#             print(f\"Error in {model_name} with {name} transformer: {e}\")\n",
    "        \n",
    "#         finally:\n",
    "#             clear_session()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(x, y_train, y_val, y_test,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
